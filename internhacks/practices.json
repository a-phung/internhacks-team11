{
    "data":
    [{  
        "id": "question-1",
        "question": "How to ensure our training dataset meets our users’ needs?",
        "topic": "Topic 02 - Translate user needs into data needs",
        "answer": "<p>Datasets that can be used to train AI models contain <b>examples</b>, which contain one or more <b>features</b>, and possibly <b>labels</b>.</p><p>When deciding which examples, features, and labels are needed to train your ML model, work through your data needs on a conceptual level, as shown below.</p><p><b>Create a dataset specification</b></p><p>Just as you would create a product specification before starting work on a new feature or product, consider writing a document to specify your data needs. Use the problem statement that you defined from the user needs to define the dataset that you will need.</p><p><b>Get the data</b></p><p>Once you’ve defined the data requirements, you’ll start gathering the data. A good place to start is to determine if there are any existing datasets that you can reuse.If you decide to acquire an existing dataset, make sure that you have the following information:</p><ul><li>Is this data appropriate for your users and use case?</li><li>How was the data collected?</li><li>Which transformations were applied to it?</li><li>Do you need to augment it with additional data sources to be useful?</li></ul><p><b>Balance underfitting & overfitting</b></p><p>To build products to work in one context, use datasets that are expected to reliably reflect that context.</p><p>If your training data isn’t properly suited to the context, you also increase the risk of overfitting or underfitting your training set. Overfitting means the ML model is tailored too specifically to the training data. If an ML model has overfit the training data, it can make great predictions on the training data but performs worse on the test set or when given new data.</p><p>Models can also make poor predictions due to underfitting, where a model hasn’t properly captured the complexity of the relationships among the training dataset features and therefore can’t make good predictions with training data or with new data.</p><p><b>Commit to fairness</b></p><p>At every stage of development, human bias can be introduced into the ML model. Here are some examples of how ML systems can fail users:</p><ul><li>Representational harm, when a system amplifies or reflects negative stereotypes about particular groups.</li><li>Opportunity denial, when systems make predictions and decisions that have real-life consequences and lasting impacts on individuals’ access to opportunities, resources, and overall quality of life.</li><li>Disproportionate product failure, when a product doesn’t work or gives skewed outputs more frequently for certain groups of users.</li><li>Harm by disadvantage, when a system infers disadvantageous associations between certain demographic characteristics and user behaviors or interests.</li></ul><p><b>Use data that applies to different groups of users</b></p><p>Your training data should reflect the diversity and cultural context of the people who will use it. In doing so, note that to properly train your model, you might need to collect data from equal proportions of different user groups that might not exist in equal proportions in the real world.</p>"
    },
    {
        "id": "question-2",
        "question": "Should we use an existing training dataset or develop our own?",
        "topic": "Topic 03 - Source your data responsibly",
        "answer": "<p><b>Use existing datasets</b></p><p>It may not be possible to build your dataset from scratch. As an alternative, you may need to use existing data from sources such as Google Cloud AutoML, Google Dataset Search, Google AI datasets, or Kaggle.</p><p>Before using an existing dataset, take the time to thoroughly explore it using tools like ours to better understand any gaps or biases. Real data is often messy, so you should expect to spend a fair amount of time cleaning it up.</p><p><b>Build your own dataset</b></p><p>When creating your own dataset, it’s wise to start by observing someone who is an expert in the domain your product aims to serve. Instead of one-off partnerships with domain experts, strive for ongoing collaborations with domain experts throughout the project lifecycle.</p><p>Once you’ve gathered potential sources for your dataset, spend some time getting to know your data. You’ll need to go through the following steps:</p><ul><li>Identify your data source(s).</li><li>Review how often your data source(s) are refreshed.</li><li>Inspect the features’ possible values, units, and data types.</li><li>Identify any outliers, and investigate whether they’re actual outliers or due to errors in the data.</li></ul><p>Understanding where your dataset came from and how it was collected will help you discover potential issues.</p><p><b>Capture the real world</b></p><p>Make sure that your input data is as similar as possible to real-world data to avoid model failure in production. </p><p>In many cases, you’ll find that the data captured by users can be starkly different from that in your training dataset. Instead of striving for a very “clean” dataset that contains only very high-resolution images for an image classification problem, or only correctly formatted and typo-free movie reviews for a sentiment analysis problem, allow some ‘noise’ into your training dataset.</p><p><b>Prepare a data maintenance plan</b></p><p>If you want to create a dataset, consider whether you can maintain it and if you can afford the risks of something going wrong. Focus on these tasks to maintain dataset quality:</p><ul><li><b>Preventive maintenance</b>: Prevent problems before they occur. Store your dataset in a stable repository that provides different levels of access and stable identifiers.</li><li><b>Adaptive maintenance</b>: Preserve the dataset while the real world changes. Decide which properties of the dataset should be preserved, and keep this data updated over time. </li><li><b>Corrective maintenance</b>: Fix errors. Problems can occur as a result of data cascades. Keep a detailed, human-readable log of everything that you change in the dataset.</li></ul>"
    },
    {
        "id": "question-3",
        "question": "How can we ensure that the data quality is high?",
        "topic": "Topic 03 - Translate User Needs into Data Needs",
        "answer": "<p>Data is critical to AI, but the time and resources invested in model development and performance often outweigh those spent on data quality.This can lead to significant downstream effects, called “data cascades”. Here’s an example of a data cascade:</p><p>Let’s say you’re developing an app that allows the user to upload the picture of a plant, and then displays a prediction for the plant’s type, and whether it is safe for humans and pets to touch and eat it.</p><p>When you prepared a dataset to train the image classification model, you used mostly images of plants native to North America because you found a dataset that was already labeled and easy to use to train the model. Once you released the app, however, you found out that many users were reporting plant detection errors in South America. </p><p>This is an example of a data cascade, as the effects of the mismatch between training data and user data in the real world were delayed. This could have been avoided by including images of plants native to South America when developing the AI model, or releasing the app to users in North America only.</p><p>It’s best to plan to use high-quality data from the beginning, this can help you detect issues earlier. High-quality data can be defined as:</p><ul><li>Accurately representing a real-world phenomenon or entity</li><li>Collected, stored, and used responsibly</li><li>Reproducible</li><li>Maintainable over time</li><li>Reusable across relevant applications</li><li>Having empirical and explanatory power</li></ul>"
    },
    {
        "id": "question-4",
        "question": "How can we prevent errors and bias in datasets when generating labels?",
        "topic": "Topic 04 - Prepare and document your data",
        "answer": "<p><b>Split your dataset into training and test sets</b></p><p>And finally, you’ll need to split the data into training and test sets. A typical split of your dataset could result in: 60% for training, and 40% for testing.</p><p><b>Analyze and prepare your data</b></p><p>An important step in all model training pipelines is handling “dirty” or inconsistent data. Data cleaning often consists of two stages: detecting and addressing issues.</p><p>Data cleaning and analysis require iteration. Analyzing and visualizing data may help identify issues in your dataset, possibly requiring further cleaning.</p><p><b>Document your data</b></p><p>Having a record of your dataset’s sources, a list of operations and transformations that have been applied to it, its history over time, and recommended uses can help when you do the following:</p><ul><li>Share your dataset with colleagues on your team or on another team</li><li>Review whether you can use or publish your dataset</li><li>Compare multiple datasets side by side</li><li>Use the data responsibly in to train a model for an AI-powered product</li><li>Interpret the behavior of an AI model that you trained with the data</li><li>Maintain the dataset for teams and systems that rely on it</li></ul><p>Data Cards are a form of documentation for datasets, and include information that can help answer the following questions about the data:</p><ul><li>What does it represent?</li><li>What does it look like?</li><li>Where does it come from?</li><li>How was it prepared?</li><li>Can it be used for a specific use case?</li><li>How should it be used responsibly?</li></ul>"
    },
    {
        "id": "question-5",
        "question": "Are we treating data workers fairly?",
        "topic": "Topic 05 - Translate User Needs into Data Needs",
        "answer": "<p>Data is critical to AI, but the time and resources invested in model development and performance often outweigh those spent on data quality.</p><p>This can lead to significant downstream effects throughout the development pipeline, which we call “data cascades”. Here’s a hypothetical example of a data cascade:</p><ul><li>Let’s say you’re developing an app that allows the user to upload the picture of a plant, and then displays a prediction for the plant’s type, and whether it is safe for humans and pets to touch and eat it.</li><li>When you prepared a dataset to train the image classification model, you used mostly images of plants native to North America because you found a dataset that was already labeled and easy to use to train the model.</li><li>Once you released the Plant Pal app, however, you found out that many users were reporting plant detection errors in South America.</li></ul>"
    },
    {
        "id": "question-6",
        "question": "How do I responsibly build my dataset?",
        "topic": "Topic 03 - Source your data responsibly",
        "answer": "<p><b>Use existing datasets</b></p><p>It may not be possible to build your dataset from scratch. As an alternative, you may need to use existing data from sources such as Google Cloud AutoML, Google Dataset Search, Google AI datasets, or Kaggle.</p><p>Before using an existing dataset, take the time to thoroughly explore it using tools like ours to better understand any gaps or biases. Real data is often messy, so you should expect to spend a fair amount of time cleaning it up.</p><p><b>Build your own dataset</b></p><p>When creating your own dataset, it’s wise to start by observing someone who is an expert in the domain your product aims to serve. Instead of one-off partnerships with domain experts, strive for ongoing collaborations with domain experts throughout the project lifecycle.</p><p>Once you’ve gathered potential sources for your dataset, spend some time getting to know your data. You’ll need to go through the following steps:</p><ul><li>Identify your data source(s).</li><li>Review how often your data source(s) are refreshed.</li><li>Inspect the features’ possible values, units, and data types.</li><li>Identify any outliers, and investigate whether they’re actual outliers or due to errors in the data.</li></ul><p>Understanding where your dataset came from and how it was collected will help you discover potential issues.</p><p><b>Capture the real world</b></p><p>Make sure that your input data is as similar as possible to real-world data to avoid model failure in production. </p><p>In many cases, you’ll find that the data captured by users can be starkly different from that in your training dataset. Instead of striving for a very “clean” dataset that contains only very high-resolution images for an image classification problem, or only correctly formatted and typo-free movie reviews for a sentiment analysis problem, allow some ‘noise’ into your training dataset.</p><p><b>Prepare a data maintenance plan</b></p><p>If you want to create a dataset, consider whether you can maintain it and if you can afford the risks of something going wrong. Focus on these tasks to maintain dataset quality:</p><ul><li><b>Preventive maintenance</b>: Prevent problems before they occur. Store your dataset in a stable repository that provides different levels of access and stable identifiers.</li><li><b>Adaptive maintenance</b>: Preserve the dataset while the real world changes. Decide which properties of the dataset should be preserved, and keep this data updated over time. </li><li><b>Corrective maintenance</b>: Fix errors. Problems can occur as a result of data cascades. Keep a detailed, human-readable log of everything that you change in the dataset.</li></ul>"
    },
    {
        "id": "question-7",
        "question": "What metrics will we use to determine if our tuning is successful?",
        "topic": "Topic 07 - Translate User Needs into Data Needs",
        "answer": "<p>Data is critical to AI, but the time and resources invested in model development and performance often outweigh those spent on data quality.</p><p>This can lead to significant downstream effects throughout the development pipeline, which we call “data cascades”. Here’s a hypothetical example of a data cascade:</p><ul><li>Let’s say you’re developing an app that allows the user to upload the picture of a plant, and then displays a prediction for the plant’s type, and whether it is safe for humans and pets to touch and eat it.</li><li>When you prepared a dataset to train the image classification model, you used mostly images of plants native to North America because you found a dataset that was already labeled and easy to use to train the model.</li><li>Once you released the Plant Pal app, however, you found out that many users were reporting plant detection errors in South America.</li></ul>"
    }
]
}
    
    

